{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "\n",
    "class SimpleSummarize:\n",
    "    def __init__(self, filename=None, k=None):\n",
    "        self.txt = None\n",
    "        self.word_tokens = None\n",
    "        self.sent_tokens = None\n",
    "        self.word_freq = None\n",
    "        self.freq_dist = {}\n",
    "        self.sent_scores = {}\n",
    "        self.top_sents = None\n",
    "        self.max_len = 40\n",
    "        self.summary = ''\n",
    "        self.scores = []\n",
    "        self.english_stopwords = set(stopwords.words('english')) | stop_words\n",
    "        if filename and k:\n",
    "            self.load_file_from_disk(filename)\n",
    "            self.tokenize()\n",
    "            self.word_freq_dist()\n",
    "            self.score_sentences()\n",
    "            self.summarize(k)\n",
    "    \n",
    "    def load_file_from_disk(self, filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            self.txt = file.read().replace(\"\\n\", \" \")\n",
    "            self.txt = self.txt.replace(\"\\'\",\"\")\n",
    "    \n",
    "    def tokenize(self):\n",
    "        self.word_tokens = self.tokenizer(self.txt)\n",
    "        #self.sent_tokens = self.simple_sent_tokenizer(self.txt)\n",
    "        self.sent_tokens = sent_tokenize(self.txt)\n",
    "\n",
    "    def simple_sent_tokenizer(self, s):\n",
    "        sents = []\n",
    "        for sent in s.split('.'):\n",
    "            sents.append(sent.strip())\n",
    "        return sents\n",
    "        \n",
    "    def tokenizer(self,txt):\n",
    "        txt = txt.lower()\n",
    "        word_tokens = word_tokenize(txt.lower())\n",
    "        word_tokens = [w for w in word_tokens if w not in self.english_stopwords and re.match('[a-zA-Z-][a-zA-Z-]{2,}', w)]\n",
    "        return word_tokens\n",
    "    \n",
    "    def word_freq_dist(self):\n",
    "        self.word_freq = nltk.FreqDist(self.word_tokens)\n",
    "        most_freq_count = max(self.word_freq.values())\n",
    "        for k,v in self.word_freq.items():\n",
    "            self.freq_dist[k] = v/most_freq_count\n",
    "    \n",
    "    def score_sentences(self):\n",
    "        for sent in self.sent_tokens:\n",
    "            words = self.tokenizer(sent)\n",
    "            for word in words:\n",
    "                if word.lower() in self.freq_dist.keys():\n",
    "                    if len(words) < self.max_len:\n",
    "                        # if key does not exist add it and the freq_dist for the first word\n",
    "                        if sent not in self.sent_scores.keys():\n",
    "                            self.sent_scores[sent] = self.freq_dist[word.lower()]\n",
    "                        else: \n",
    "                            # the key exists and we just add the freq_dist of the following words. \n",
    "                            # We are just summing up the freq_dists for the sentence\n",
    "                            self.sent_scores[sent] += self.freq_dist[word.lower()]\n",
    "    \n",
    "    def summarize(self, k):\n",
    "        self.top_sents = Counter(self.sent_scores)\n",
    "        for t in self.top_sents.most_common(k):\n",
    "            self.summary += t[0].strip()+'. '\n",
    "            self.scores.append((t[1],t[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Judiciary Committee hearing is the latest sign that House Democrats are moving forward with impeachment proceedings against the President following the two-month investigation led by the House Intelligence Committee into allegations that Trump pushed Ukraine to investigate his political rivals while a White House meeting and $400 million in security aid were withheld from Kiev.. The House Judiciary Committee has invited President Donald Trump or his counsel to participate in the panels first impeachment hearing next week as the House moves another step closer to impeaching the President.. READ: Judiciary Chairman&#39;s invite to Trump and his lawyers to take part in upcoming impeachment hearings The hearing announcement comes as the Intelligence Committee plans to release its report summarizing the findings of its investigation to the House Judiciary Committee soon after Congress returns from its Thanksgiving recess next week.. '"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foo = SimpleSummarize()\n",
    "# foo.load_file_from_disk(\"CNNImpeachmentArticle.txt\")\n",
    "# foo.tokenize()\n",
    "# foo.word_freq_dist()\n",
    "# foo.score_sentences()\n",
    "# foo.summarize(3)\n",
    "# foo.summary\n",
    "foo = SimpleSummarize(filename=\"CNNImpeachmentArticle.txt\", k=3)\n",
    "foo.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "https://nlpforhackers.io/topic-modeling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Algorithms\n",
    "\n",
    "There are several algorithms for doing topic modeling. The most popular ones include\n",
    "\n",
    "LDA – Latent Dirichlet Allocation – The one we’ll be focusing in this tutorial. Its foundations are Probabilistic Graphical Models\n",
    "\n",
    "LSA or LSI – Latent Semantic Analysis or Latent Semantic Indexing – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra\n",
    "\n",
    "NMF – Non-Negative Matrix Factorization – Based on Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "from gensim import models, corpora, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "STOPWORDS = set(stopwords.words('english')) | stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_from_disk(data_dir):\n",
    "    text_data_list = []\n",
    "    file_list = glob(pathname=data_dir + '/*')\n",
    "    for file in file_list: \n",
    "        with open(file, \"r\") as f:\n",
    "            text_data_list.append(f.read())\n",
    "    return text_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data = load_files_from_disk('articles/')\n",
    "len(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in article_data:\n",
    "    tokenized_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Gensim Dictionary - assocation word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "# for k,v in dictionary.items():\n",
    "#     print((k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 1)]\n",
      "[(0, 1), (18, 1), (39, 1), (49, 1), (52, 2), (70, 2), (71, 1), (74, 1), (81, 1), (84, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[0][:5])\n",
    "print(corpus[20][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "#Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "LDA Model:\n",
      "Topic #0: 0.006*\"like\" + 0.004*\"said\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"people\" + 0.003*\"years\" + 0.002*\"way\" + 0.002*\"going\" + 0.002*\"know\" + 0.002*\"make\"\n",
      "Topic #1: 0.005*\"new\" + 0.005*\"like\" + 0.004*\"people\" + 0.003*\"time\" + 0.003*\"game\" + 0.002*\"said\" + 0.002*\"make\" + 0.002*\"years\" + 0.002*\"data\" + 0.002*\"football\"\n",
      "Topic #2: 0.004*\"like\" + 0.004*\"new\" + 0.004*\"people\" + 0.003*\"time\" + 0.003*\"way\" + 0.003*\"said\" + 0.003*\"work\" + 0.003*\"make\" + 0.002*\"years\" + 0.002*\"game\"\n",
      "Topic #3: 0.005*\"like\" + 0.004*\"time\" + 0.004*\"people\" + 0.004*\"new\" + 0.003*\"said\" + 0.003*\"way\" + 0.003*\"work\" + 0.003*\"game\" + 0.002*\"know\" + 0.002*\"team\"\n",
      "Topic #4: 0.005*\"people\" + 0.005*\"like\" + 0.004*\"new\" + 0.003*\"data\" + 0.003*\"way\" + 0.003*\"time\" + 0.002*\"use\" + 0.002*\"years\" + 0.002*\"know\" + 0.002*\"church\"\n",
      "Topic #5: 0.006*\"like\" + 0.005*\"new\" + 0.003*\"people\" + 0.003*\"make\" + 0.003*\"time\" + 0.002*\"said\" + 0.002*\"team\" + 0.002*\"rice\" + 0.002*\"years\" + 0.002*\"know\"\n",
      "Topic #6: 0.006*\"like\" + 0.004*\"time\" + 0.003*\"said\" + 0.003*\"new\" + 0.003*\"people\" + 0.003*\"make\" + 0.003*\"want\" + 0.002*\"game\" + 0.002*\"years\" + 0.002*\"know\"\n",
      "Topic #7: 0.007*\"like\" + 0.004*\"new\" + 0.003*\"time\" + 0.003*\"make\" + 0.003*\"people\" + 0.002*\"said\" + 0.002*\"way\" + 0.002*\"use\" + 0.002*\"world\" + 0.002*\"need\"\n",
      "Topic #8: 0.005*\"time\" + 0.005*\"people\" + 0.005*\"like\" + 0.004*\"said\" + 0.003*\"make\" + 0.003*\"way\" + 0.002*\"new\" + 0.002*\"church\" + 0.002*\"work\" + 0.002*\"good\"\n",
      "Topic #9: 0.005*\"like\" + 0.004*\"people\" + 0.004*\"said\" + 0.004*\"rice\" + 0.004*\"new\" + 0.004*\"pixels\" + 0.003*\"image\" + 0.003*\"time\" + 0.003*\"way\" + 0.003*\"years\"\n",
      "===================================================================================================================\n",
      "LSI Model:\n",
      "Topic #0: -0.235*\"like\" + -0.215*\"game\" + -0.195*\"new\" + -0.195*\"rice\" + -0.193*\"said\" + -0.173*\"time\" + -0.165*\"people\" + -0.164*\"football\" + -0.132*\"team\" + -0.125*\"way\"\n",
      "Topic #1: -0.588*\"rice\" + -0.279*\"goodell\" + -0.221*\"ravens\" + -0.167*\"video\" + -0.144*\"ray\" + 0.135*\"like\" + 0.130*\"people\" + -0.130*\"team\" + -0.120*\"said\" + -0.110*\"bisciotti\"\n",
      "Topic #2: -0.383*\"game\" + -0.324*\"football\" + 0.187*\"people\" + -0.175*\"players\" + 0.169*\"rice\" + 0.149*\"company\" + 0.147*\"companies\" + 0.132*\"engineers\" + 0.115*\"google\" + -0.112*\"games\"\n",
      "Topic #3: -0.426*\"engineers\" + 0.225*\"companies\" + 0.222*\"company\" + 0.190*\"blitzscaling\" + -0.175*\"software\" + 0.170*\"google\" + -0.165*\"time\" + 0.158*\"business\" + -0.152*\"work\" + 0.143*\"uber\"\n",
      "Topic #4: 0.458*\"pixels\" + 0.376*\"image\" + 0.241*\"images\" + 0.238*\"sharing\" + 0.238*\"facebook\" + -0.214*\"engineers\" + 0.148*\"ideal\" + 0.128*\"size\" + 0.121*\"twitter\" + 0.120*\"wide\"\n",
      "Topic #5: -0.376*\"flynn\" + -0.371*\"said\" + 0.322*\"engineers\" + 0.261*\"football\" + 0.148*\"software\" + 0.121*\"game\" + -0.116*\"nba\" + 0.106*\"rice\" + -0.100*\"basketball\" + -0.098*\"really\"\n",
      "Topic #6: 0.261*\"people\" + -0.258*\"flynn\" + -0.245*\"pixels\" + -0.199*\"engineers\" + -0.188*\"image\" + 0.168*\"targeting\" + -0.163*\"said\" + 0.150*\"church\" + -0.140*\"sharing\" + 0.136*\"football\"\n",
      "Topic #7: 0.316*\"football\" + 0.266*\"flynn\" + 0.186*\"game\" + -0.154*\"new\" + -0.148*\"tif\" + -0.141*\"week\" + -0.130*\"yards\" + 0.123*\"facebook\" + 0.122*\"targeting\" + 0.121*\"people\"\n",
      "Topic #8: -0.216*\"facebook\" + -0.202*\"targeting\" + 0.194*\"football\" + 0.152*\"pixels\" + 0.149*\"american\" + 0.132*\"court\" + 0.126*\"image\" + 0.120*\"tif\" + -0.112*\"new\" + 0.101*\"law\"\n",
      "Topic #9: -0.702*\"church\" + -0.243*\"like\" + -0.202*\"website\" + -0.142*\"headline\" + 0.132*\"people\" + 0.131*\"facebook\" + 0.130*\"targeting\" + -0.105*\"site\" + -0.082*\"websites\" + 0.080*\"american\"\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 115)\n",
    "print(\"LDA Model:\")\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\"%idx, lda_model.print_topic(idx,10))\n",
    "print(\"=\" * 115)\n",
    "print(\"LSI Model:\")\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\"%idx, lsi_model.print_topic(idx,10))\n",
    "print(\"=\" * 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -1.461097125023915), (1, 0.5769466544499854), (2, -0.679132031595229), (3, 0.08706842861130795), (4, 0.22426330811234568), (5, 0.27258391940017535), (6, 0.7620872250218782), (7, 0.9151602360814599), (8, 0.10802040547313593), (9, -0.6742140436068658)]\n",
      "[(9, 0.9356761)]\n"
     ]
    }
   ],
   "source": [
    "t = \"I like football. It's cool to watch and I like the players and I like people. Football offense defense field goal\"\n",
    "bow = dictionary.doc2bow(clean_text(t))\n",
    "print(lsi_model[bow])\n",
    "print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = lda_index[lda_model[bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.999387), (13, 0.999387), (18, 0.999387), (22, 0.999387), (23, 0.999387), (24, 0.999387), (27, 0.999387), (35, 0.999387), (63, 0.999387), (72, 0.999387)]\n"
     ]
    }
   ],
   "source": [
    "print(sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Can't Even: Stories dedicated to fashion in 2009, when everything was \"bananas.\" What we talk about when we talk about 2009. Photo-Illustration: by Stevie Remsberg; Photo by Dennis Valle\n",
      "\n",
      "Your own time: You love to see it! Good luck trying. We’re suspended in it, swimming through Jell-O, with all the clarity that suggests. Sometimes you can only see where you are by looking at where you were. Ten years ago, when I was a young fashion writer, things looked different, but not dark-to-light different. “President Trump” existed — as a Simpsons character. I was sure we’d never give up BBM. My in-box, that searchable tomb, from September 2009: a birthday reminder (from Friendster), notice of a Netflix delivery (on DVD), a newsletter from Refinery 29, their list of the best models who blog. Recognizable, but off. It was today in embryo, today 1.0: a huge terrain to cover in a mere decade, but not an impossible one. Blink, and you’re here. Thus does the mold of history gel and set.\n",
      "\n",
      "This week, we’re looking back to the year 2009 in fashion and the culture of fashion, a time that feels, in hindsight, painfully naive — really, almost sweet. Sure, there was a recession, with real anxiety and even realer layoffs at all the major magazines. But the near future was so unknowable as to feel absurd now. Fears about bloggers and front rows now look like the calm before the digital storm. This was before Instagram, before influencers, and for all intents and purposes, before much thought in fashion went into diversity or inclusivity (the famous black issue of Vogue Italia had come out the summer before). If anyone discussed “direct to consumer” shopping, it wasn’t with the consumer, or at least not this one. All of this was a nascent dream then. But it’s the disrupted then, which has brought us to the disrupted now.\n",
      "\n",
      "In September 2009, I was working for a website — funny how this happens — that no longer exists. I was struggling manfully to elbow my way into a world that was chang\n"
     ]
    }
   ],
   "source": [
    "document_id, similarity = sims[0]\n",
    "print(article_data[document_id][:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn for Topic Modeling\n",
    "\n",
    "scikit-learn offers an NMF model in addition to LDA and LSI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
