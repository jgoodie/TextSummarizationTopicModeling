{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from flask import Flask\n",
    "from flask import request, jsonify\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import logging\n",
    "import urllib3\n",
    "import requests\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSummarize:\n",
    "    def __init__(self, filename=None, k=None):\n",
    "        self.txt = None\n",
    "        self.word_tokens = None\n",
    "        self.sent_tokens = None\n",
    "        self.word_freq = None\n",
    "        self.freq_dist = {}\n",
    "        self.sent_scores = {}\n",
    "        self.top_sents = None\n",
    "        self.max_len = 40\n",
    "        self.summary = ''\n",
    "        self.scores = []\n",
    "        self.english_stopwords = set(stopwords.words('english')) | stop_words\n",
    "        if filename and k:\n",
    "            self.load_file_from_disk(filename)\n",
    "            self.tokenize()\n",
    "            self.word_freq_dist()\n",
    "            self.score_sentences()\n",
    "            self.summarize(k)\n",
    "    \n",
    "    def load_file_from_disk(self, filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            self.txt = file.read().replace(\"\\n\", \" \")\n",
    "            self.txt = self.txt.replace(\"\\'\",\"\")\n",
    "    \n",
    "    def tokenize(self):\n",
    "        self.word_tokens = self.tokenizer(self.txt)\n",
    "        #self.sent_tokens = self.simple_sent_tokenizer(self.txt)\n",
    "        self.sent_tokens = sent_tokenize(self.txt)\n",
    "\n",
    "    def simple_sent_tokenizer(self, s):\n",
    "        sents = []\n",
    "        for sent in s.split('.'):\n",
    "            sents.append(sent.strip())\n",
    "        return sents\n",
    "        \n",
    "    def tokenizer(self,txt):\n",
    "        txt = txt.lower()\n",
    "        word_tokens = word_tokenize(txt.lower())\n",
    "        word_tokens = [w for w in word_tokens if w not in self.english_stopwords and re.match('[a-zA-Z-][a-zA-Z-]{2,}', w)]\n",
    "        return word_tokens\n",
    "    \n",
    "    def word_freq_dist(self):\n",
    "        self.word_freq = nltk.FreqDist(self.word_tokens)\n",
    "        most_freq_count = max(self.word_freq.values())\n",
    "        for k,v in self.word_freq.items():\n",
    "            self.freq_dist[k] = v/most_freq_count\n",
    "    \n",
    "    def score_sentences(self):\n",
    "        for sent in self.sent_tokens:\n",
    "            words = self.tokenizer(sent)\n",
    "            for word in words:\n",
    "                if word.lower() in self.freq_dist.keys():\n",
    "                    if len(words) < self.max_len:\n",
    "                        # if key does not exist add it and the freq_dist for the first word\n",
    "                        if sent not in self.sent_scores.keys():\n",
    "                            self.sent_scores[sent] = self.freq_dist[word.lower()]\n",
    "                        else: \n",
    "                            # the key exists and we just add the freq_dist of the following words. \n",
    "                            # We are just summing up the freq_dists for the sentence\n",
    "                            self.sent_scores[sent] += self.freq_dist[word.lower()]\n",
    "    \n",
    "    def summarize(self, k):\n",
    "        self.top_sents = Counter(self.sent_scores)\n",
    "        for t in self.top_sents.most_common(k):\n",
    "            self.summary += t[0].strip()+'. '\n",
    "            self.scores.append((t[1],t[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = SimpleSummarize()\n",
    "# foo.load_file_from_disk(\"CNNImpeachmentArticle.txt\")\n",
    "# foo.tokenize()\n",
    "# foo.word_freq_dist()\n",
    "# foo.score_sentences()\n",
    "# foo.summarize(3)\n",
    "# foo.summary\n",
    "foo = SimpleSummarize(filename=\"CNNImpeachmentArticle.txt\", k=3)\n",
    "foo.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "https://towardsdatascience.com/text-summarization-in-python-76c0a41f0dc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Judiciary Chairman Jerry Nadler sent a letter to Trump on Tuesday notifying him of the hearing and inviting the President or his counsel to participate, including asking questions of the witnesses.\n",
      "The Judiciary Committee hearing is the latest sign that House Democrats are moving forward with impeachment proceedings against the President following the two-month investigation led by the House Intelligence Committee into allegations that Trump pushed Ukraine to investigate his political rivals while a White House meeting and $400 million in security aid were withheld from Kiev.\n",
      "Nadler asked Trump to respond by Sunday on whether the White House wanted to participate in the hearings, as well as who would act as the President's counsel for the proceedings.\n"
     ]
    }
   ],
   "source": [
    "with open(\"CNNImpeachmentArticle.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "print(summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"We all know that music is a powerful influencer. A movie without a soundtrack doesn’t provoke the same emotional journey. A workout without a pump-up anthem can feel like a drag. But is there a way to quantify these reactions? And if so, could they be reverse-engineered and put to use?\n",
    "\n",
    "In a new paper, researchers at the University of Southern California mapped out how things like  pitch, rhythm, and harmony induce different types of brain activity, physiological reactions (heat, sweat, and changes in electrical response), and emotions (happiness or sadness), and how machine learning could use those relationships to predict how people might respond to a new piece of music. The results, presented at a conference last week on the intersections of computer science and art, show how we may one day be able to engineer targeted musical experiences for purposes ranging from therapy to movies.\n",
    "\n",
    "The research is part of the lab’s broader goal to understand how different forms of media, such as films and TV ads as well as music, affect people’s bodies and brains. “Once we understand how media can affect your various emotions, then we can try to productively use it for actually supporting or enhancing human experiences,” says Shrikanth Narayanan, a professor at USC and the principal investigator in the lab.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a new paper, researchers at the University of Southern California mapped out how things like  pitch, rhythm, and harmony induce different types of brain activity, physiological reactions (heat, sweat, and changes in electrical response), and emotions (happiness or sadness), and how machine learning could use those relationships to predict how people might respond to a new piece of music.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"\"\"Republican South Carolina Senator Lindsey Graham is statistically tied with Democratic challenger Jaime Harrison, with the staunch pro-Trump incumbent seeing his favorability ratings plummet among independent voters.\n",
    "\n",
    "Graham, who was first elected to the U.S. Senate in 2002, is clinging to a 2-percentage point lead over Harrison, 47 to 45 percent, with nearly 10 percent of voters surveyed still remaining undecided about their 2020 vote. As the Change Research/Post and Courier newspaper poll notes, Graham performs poorly in hypothetical November election matchups as well as with voters who do not identify solely as Republicans. The statistical dead heat between Graham and Harrison, who has pulled in record fundraising in recent weeks, comes as the GOP senator has become one of President Donald Trump's most outspoken supporters during the ongoing impeachment inquiry.\n",
    "\n",
    "Trump won the traditionally-Republican state of South Carolina in the 2016 presidential election by a margin of nearly 15 percentage points over Democrat Hillary Clinton.\n",
    "\n",
    "\"[Senator Lindsey Graham] looks extremely vulnerable against Democratic contender Jaime Harrison,\" the South Carolina pollsters noted among their key findings. \"While South Carolina does not support impeaching President Trump, a majority of voters would like Senator Graham to approach the impeachment inquiry with an open mind, rather than leap to the president's defense before hearing evidence.\"\n",
    "\n",
    "Graham has not always been such a staunch defender of Trump, with the longtime Republican senator infamously remarking during the 2016 primary, \"If we nominate Trump, we will get destroyed......and we will deserve it.\"\n",
    "\n",
    "\n",
    "Lindsey Graham\n",
    "✔\n",
    "@LindseyGrahamSC\n",
    "If we nominate Trump, we will get destroyed.......and we will deserve it.\n",
    "\n",
    "162K\n",
    "1:03 PM - May 3, 2016\n",
    "Twitter Ads info and privacy\n",
    "143K people are talking about this\n",
    "However, Graham made recent comments that he doubts the president so little that he doesn't intend on being a \"fair juror,\" if and when the House votes to send the articles of impeachment over to the Senate.\n",
    "\n",
    "Harrison told residents of Greenville, South Carolina on Saturday that Graham is \"not worthy of this state ... the winds of change are blowing my friends,\" the Greenville News reported. Responding to the neck-and-neck South Carolina poll on Twitter, Harrison asked his supporters and potential voters to remember that any campaign is possible in the current political environment.\n",
    "\n",
    "\"Running against Senator Graham is a tough climb, but it's also a hill worth climbing. I've faced things folks have deemed impossible my entire life, and this is yet another journey where I prove that in America, the impossible is always possible,\" Harrison wrote.\n",
    "\n",
    "Harrison declared his candidacy in June and has previously served as chair and senior counselor at the Democratic National Committee, as well as heading the Democratic Party of South Carolina. The Yale University graduate and South Carolina native was also an advisor to Congressman James Clyburn.\n",
    "\n",
    "\"It's an uphill battle, no question, but Jaime is uniquely qualified,\" said House Majority Whip Clyburn, told The Washington Post last week. \"He has the kind of life experiences that allow him to really connect with ordinary people.\"\n",
    "\n",
    "The Change Research surveyed 998 likely general election voters in the state of South Carolina between December 6-11. The poll found only 38 percent of likely voters said they will \"definitely\" or \"probably\" vote for Graham next November. Trump has higher favorability numbers than Graham, with the Republican senator holding onto the support of less than half of those surveyed. In terms of the presidential election, former Vice President Joe Biden leads the Democratic primary with 27 percent, followed by Vermont Senator Bernie Sanders with 20 percent of the vote.\"\"\"\n",
    "\n",
    "text3 = text3.strip().replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Republican South Carolina Senator Lindsey Graham is statistically tied with Democratic challenger Jaime Harrison, with the staunch pro-Trump incumbent seeing his favorability ratings plummet among independent voters.  Graham, who was first elected to the U.S. Senate in 2002, is clinging to a 2-percentage point lead over Harrison, 47 to 45 percent, with nearly 10 percent of voters surveyed still remaining undecided about their 2020 vote. As the Change Research/Post and Courier newspaper poll notes, Graham performs poorly in hypothetical November election matchups as well as with voters who do not identify solely as Republicans. The statistical dead heat between Graham and Harrison, who has pulled in record fundraising in recent weeks, comes as the GOP senator has become one of President Donald Trumps most outspoken supporters during the ongoing impeachment inquiry.  Trump won the traditionally-Republican state of South Carolina in the 2016 presidential election by a margin of nearly 15 percentage points over Democrat Hillary Clinton.  \"[Senator Lindsey Graham] looks extremely vulnerable against Democratic contender Jaime Harrison,\" the South Carolina pollsters noted among their key findings. \"While South Carolina does not support impeaching President Trump, a majority of voters would like Senator Graham to approach the impeachment inquiry with an open mind, rather than leap to the presidents defense before hearing evidence.\"  Graham has not always been such a staunch defender of Trump, with the longtime Republican senator infamously remarking during the 2016 primary, \"If we nominate Trump, we will get destroyed......and we will deserve it.\"   Lindsey Graham ✔ @LindseyGrahamSC If we nominate Trump, we will get destroyed.......and we will deserve it.  162K 1:03 PM - May 3, 2016 Twitter Ads info and privacy 143K people are talking about this However, Graham made recent comments that he doubts the president so little that he doesnt intend on being a \"fair juror,\" if and when the House votes to send the articles of impeachment over to the Senate.  Harrison told residents of Greenville, South Carolina on Saturday that Graham is \"not worthy of this state ... the winds of change are blowing my friends,\" the Greenville News reported. Responding to the neck-and-neck South Carolina poll on Twitter, Harrison asked his supporters and potential voters to remember that any campaign is possible in the current political environment.  \"Running against Senator Graham is a tough climb, but its also a hill worth climbing. Ive faced things folks have deemed impossible my entire life, and this is yet another journey where I prove that in America, the impossible is always possible,\" Harrison wrote.  Harrison declared his candidacy in June and has previously served as chair and senior counselor at the Democratic National Committee, as well as heading the Democratic Party of South Carolina. The Yale University graduate and South Carolina native was also an advisor to Congressman James Clyburn.  \"Its an uphill battle, no question, but Jaime is uniquely qualified,\" said House Majority Whip Clyburn, told The Washington Post last week. \"He has the kind of life experiences that allow him to really connect with ordinary people.\"  The Change Research surveyed 998 likely general election voters in the state of South Carolina between December 6-11. The poll found only 38 percent of likely voters said they will \"definitely\" or \"probably\" vote for Graham next November. Trump has higher favorability numbers than Graham, with the Republican senator holding onto the support of less than half of those surveyed. In terms of the presidential election, former Vice President Joe Biden leads the Democratic primary with 27 percent, followed by Vermont Senator Bernie Sanders with 20 percent of the vote.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3.replace(\"\\'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another text summarizer\n",
    "\n",
    "https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        txt = file.read()\n",
    "        sent_tokens = sent_tokenize(txt)\n",
    "        return sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros([len(sentences), len(sentences)])\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(file_name, top_n=5):\n",
    "    # set(stopwords.words('english')) | stop_words # Union operator for set() in python\n",
    "    stop_words = set(stopwords.words('english')) | sw\n",
    "    summarize_text = []\n",
    "    sentences = read_article(file_name)\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentences)    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\"\".join(ranked_sentences[i][1]))\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1 = 'This is a sentence that I wrote.'\n",
    "s2 = 'I wrote a sentence and drew a picture.'\n",
    "s3 = 'Those are some green apples.'\n",
    "print(sentence_similarity(s1, s2, stop_words))\n",
    "print(sentence_similarity(s1, s3, stop_words))\n",
    "print(sentence_similarity(s2, s3, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_article(\"impeachment_data/cnn_article.txt\")\n",
    "build_similarity_matrix(sentences, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary(\"CNNImpeachmentArticle.txt\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = 'impeachment_data/20191203_-_full_report___hpsci_impeachment_inquiry_-_20191203.pdf'\n",
    "pd_file_obj = open(pdf, 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pd_file_obj)\n",
    "num_pages = pdf_reader.getNumPages()\n",
    "full_text = ''\n",
    "for p in range(num_pages):\n",
    "    full_text = full_text + pdf_reader.getPage(p).extractText().strip().replace('\\n','')\n",
    "with open('impeachment_data/trump_impeachment_inquiry.txt', 'w+') as file:\n",
    "    file.write(full_text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impeachment_summary = SimpleSummarize(filename=\"impeachment_data/trump_impeachment_inquiry.txt\", k=3)\n",
    "impeachment_summary.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impeachment_summary.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "https://nlpforhackers.io/topic-modeling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Algorithms\n",
    "\n",
    "There are several algorithms for doing topic modeling. The most popular ones include\n",
    "\n",
    "LDA – Latent Dirichlet Allocation – The one we’ll be focusing in this tutorial. Its foundations are Probabilistic Graphical Models\n",
    "\n",
    "LSA or LSI – Latent Semantic Analysis or Latent Semantic Indexing – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra\n",
    "\n",
    "NMF – Non-Negative Matrix Factorization – Based on Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "from gensim import models, corpora, similarities\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_from_disk(data_dir):\n",
    "    text_data_list = []\n",
    "    file_list = glob(pathname=data_dir + '/*txt')\n",
    "    for file in file_list: \n",
    "        with open(file, \"r\") as f:\n",
    "            text_data_list.append(f.read())\n",
    "    return text_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryTopics:\n",
    "    def __init__(self, text_data, num_topics=4):\n",
    "        self.text_data = text_data\n",
    "        self.num_topics = num_topics\n",
    "        self.tokenized_data = []\n",
    "        self.all_topics = []\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.lda_model = None\n",
    "        self.lsi_model = None\n",
    "        self.stopwords = set(stopwords.words('english')) | stop_words\n",
    "    \n",
    "    def add_stop_words(self, stopword_list):\n",
    "        # set(list(t.stopwords) + [\"bob\", \"jerry\"])\n",
    "        self.stopwords = set(list(self.stopwords) + stopword_list)\n",
    "    \n",
    "    def text_summarize(self):\n",
    "        summaries = []\n",
    "        for text in self.text_data:\n",
    "            text_data = text.strip().replace(\"\\n\",\" \")\n",
    "            summaries.append(summarize(text_data).strip().replace(\"\\n\",\" \"))\n",
    "        return json.dumps(summaries)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        tokenized_text = word_tokenize(text.lower())\n",
    "        cleaned_text = [t for t in tokenized_text if t not in self.stopwords and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "        return cleaned_text\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        # For gensim we need to tokenize the data and filter out stopwords\n",
    "        for text in self.text_data:\n",
    "            self.tokenized_data.append(self.clean_text(text))\n",
    "        return self.tokenized_data\n",
    "    \n",
    "    def build_dictionary(self):\n",
    "        # Build a Gensim Dictionary - associate word to numeric id\n",
    "        self.dictionary = corpora.Dictionary(self.tokenized_data)\n",
    "        return self.dictionary\n",
    "    \n",
    "    def build_corpus(self):\n",
    "        # Transform the collection of texts to a numerical form\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in self.tokenized_data]\n",
    "        return self.corpus\n",
    "    \n",
    "    def build_models(self):\n",
    "        # Build the LDA model and the LSI model\n",
    "        self.lda_model = models.LdaModel(corpus=self.corpus, \n",
    "                                         num_topics=self.num_topics, \n",
    "                                         id2word=self.dictionary)\n",
    "        self.lsi_model = models.LsiModel(corpus=self.corpus, \n",
    "                                         num_topics=self.num_topics, \n",
    "                                         id2word=self.dictionary)\n",
    "    \n",
    "    def get_topics(self, num_topics=5):\n",
    "        pattern = r'\"([A-Za-z0-9_\\./\\\\-]*)\"'\n",
    "        for idx in range(self.lda_model.num_topics):\n",
    "            m = re.findall(pattern, self.lda_model.print_topic(idx,num_topics))\n",
    "            self.all_topics += m\n",
    "        for idx in range(self.lsi_model.num_topics):\n",
    "            m = re.findall(pattern, self.lsi_model.print_topic(idx,num_topics))\n",
    "            self.all_topics += m\n",
    "        return list(set(self.all_topics))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# article_data = load_files_from_disk('articles/')\n",
    "#t = SummaryTopics(article_data)\n",
    "# tokens = t.tokenize_text()\n",
    "# gensim_dict = t.build_dictionary()\n",
    "# gensim_corpus = t.build_corpus()\n",
    "# t.build_models()\n",
    "# t.get_topics()\n",
    "# set(list(t.stopwords) + [\"bob\", \"jerry\"])\n",
    "# t.add_stop_words(['bab', 'bob', 'jerry', 'george', 'elaine'])\n",
    "# t.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/dataframe_1572372176.7133894.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>url</th>\n",
       "      <th>file_name</th>\n",
       "      <th>keyword</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elephants Under Attack Have An Unlikely Ally: ...</td>\n",
       "      <td>A few years ago, Paul Allen, the co-founder of...</td>\n",
       "      <td>https://www.npr.org/2019/10/25/760487476/eleph...</td>\n",
       "      <td>2ffa358f-ee81-4df5-9444-33ef8beb8773.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artificial Intelligence Research Needs Respons...</td>\n",
       "      <td>After nearly a year of suspense and controvers...</td>\n",
       "      <td>https://www.lawfareblog.com/artificial-intelli...</td>\n",
       "      <td>0efc8114-f8d2-4435-9352-798c49f5bba5.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why We Shouldn’t Want Banks to Go All In on Ar...</td>\n",
       "      <td>Banks love to brag about how many data scienti...</td>\n",
       "      <td>https://slate.com/technology/2019/10/banks-art...</td>\n",
       "      <td>ee56040f-9325-4449-b3bc-62e7c824306d.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A face-scanning algorithm increasingly decides...</td>\n",
       "      <td>An artificial intelligence hiring system has b...</td>\n",
       "      <td>https://www.washingtonpost.com/technology/2019...</td>\n",
       "      <td>72f472de-e9f5-45b9-bc7b-49e2c8565e79.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial intelligence as a tool for deliveri...</td>\n",
       "      <td>Many of us are now familiar, as consumers, wit...</td>\n",
       "      <td>https://www.zdnet.com/article/artificial-intel...</td>\n",
       "      <td>be1dd15e-6c43-422e-97c9-bbb7f69c2cbb.txt</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>information technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Elephants Under Attack Have An Unlikely Ally: ...   \n",
       "1  Artificial Intelligence Research Needs Respons...   \n",
       "2  Why We Shouldn’t Want Banks to Go All In on Ar...   \n",
       "3  A face-scanning algorithm increasingly decides...   \n",
       "4  Artificial intelligence as a tool for deliveri...   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0  A few years ago, Paul Allen, the co-founder of...   \n",
       "1  After nearly a year of suspense and controvers...   \n",
       "2  Banks love to brag about how many data scienti...   \n",
       "3  An artificial intelligence hiring system has b...   \n",
       "4  Many of us are now familiar, as consumers, wit...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.npr.org/2019/10/25/760487476/eleph...   \n",
       "1  https://www.lawfareblog.com/artificial-intelli...   \n",
       "2  https://slate.com/technology/2019/10/banks-art...   \n",
       "3  https://www.washingtonpost.com/technology/2019...   \n",
       "4  https://www.zdnet.com/article/artificial-intel...   \n",
       "\n",
       "                                  file_name                  keyword  \\\n",
       "0  2ffa358f-ee81-4df5-9444-33ef8beb8773.txt  Artificial Intelligence   \n",
       "1  0efc8114-f8d2-4435-9352-798c49f5bba5.txt  Artificial Intelligence   \n",
       "2  ee56040f-9325-4449-b3bc-62e7c824306d.txt  Artificial Intelligence   \n",
       "3  72f472de-e9f5-45b9-bc7b-49e2c8565e79.txt  Artificial Intelligence   \n",
       "4  be1dd15e-6c43-422e-97c9-bbb7f69c2cbb.txt  Artificial Intelligence   \n",
       "\n",
       "                 category  \n",
       "0  information technology  \n",
       "1  information technology  \n",
       "2  information technology  \n",
       "3  information technology  \n",
       "4  information technology  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['information technology', 'relationships', 'economics',\n",
       "       'nature & ecology', 'family', 'beauty & fashion',\n",
       "       'art & tradition', 'religion', 'sports', 'politics', 'astrology',\n",
       "       'history', 'diy', 'science & technology', 'automobiles',\n",
       "       'business', 'legal', 'cooking', 'medical', 'education',\n",
       "       'meteorology', 'travel', 'health', 'celebrities', 'media'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ai_articles = df[df['keyword']=='Artificial Intelligence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 'information technology'}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /classify\n",
    "\n",
    "article_data = []\n",
    "for article in df_ai_articles.head(5)['file_name']:\n",
    "    file = \"../data/files/\" + article\n",
    "    with open(file, \"r\") as f:\n",
    "        article_data.append(f.read())\n",
    "        \n",
    "api_endpoint = \"/classify\"\n",
    "URI = \"https://arcane-badlands-69055.herokuapp.com\" + api_endpoint\n",
    "header = {'content-type': 'application/json'}\n",
    "r = requests.post(URI, headers=header, data=json.dumps(article_data[0]), verify=False) # <-- NOTE: Single doc\n",
    "json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': ['park',\n",
       "  'npr',\n",
       "  'bank',\n",
       "  'researchers',\n",
       "  'learning',\n",
       "  'machine',\n",
       "  'research',\n",
       "  'artificial',\n",
       "  'harm',\n",
       "  'said',\n",
       "  'intelligence',\n",
       "  'hirevue',\n",
       "  'potential',\n",
       "  'banks',\n",
       "  'elephant',\n",
       "  'like',\n",
       "  'people',\n",
       "  'company',\n",
       "  'job',\n",
       "  'openai',\n",
       "  'chikondi']}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /topics\n",
    "\n",
    "article_data = []\n",
    "for article in df_ai_articles.head(5)['file_name']:\n",
    "    file = \"../data/files/\" + article\n",
    "    with open(file, \"r\") as f:\n",
    "        article_data.append(f.read())\n",
    "        \n",
    "api_endpoint = \"/topics\"\n",
    "URI = \"https://arcane-badlands-69055.herokuapp.com\" + api_endpoint\n",
    "header = {'content-type': 'application/json'}\n",
    "r = requests.post(URI, headers=header, data=json.dumps(article_data), verify=False) # <-- Note: list of single or multiple docs \n",
    "json.loads(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"After nearly a year of suspense and controversy, any day now the team of artificial intelligence (AI) researchers at OpenAI will release the full and final version of GPT-2, a language model that can \\\\u201cgenerate coherent paragraphs and perform rudimentary reading comprehension, machine translation, question answering, and summarization\\\\u2014all without task-specific training.\\\\u201d When OpenAI first unveiled the program in February, it was capable of impressive feats: Given a two-sentence prompt about unicorns living in the Andes Mountains, for example, the program produced a coherent nine-paragraph news article.\\nAt the time, the technical achievement was newsworthy\\\\u2014but it was how OpenAI chose to release the new technology that really caused a firestorm.\\\\n\\\\nThere is a prevailing norm of openness in the machine learning research community, consciously created by early giants in the field: Advances are expected to be shared, so that they can be evaluated and so that the entire field advances.\\nOne group of commentators accused the organization of fear-mongering and exaggerating the dangers of the technology to garner attention; others suggested that the company had betrayed its core mission and should rename itself \\\\u201cClosedAI.\\\\u201d In May, OpenAI released a larger, 345M version of the model and announced that it would share 762M and 1.5B versions with limited partners who were also working on developing countermeasures to malicious uses.\\nNow, the task is to extrapolate out from the GPT-2 case study and develop consensus around responsible AI publication norms.\\\\n\\\\nA Growing Risk\\\\n\\\\nLike all technological advances, AI has benefits and drawbacks.\\nSome are identifying risks from malicious actors, ranging from individuals engaging in criminal activity and harassment, to industry exploiting users, to states and others engaged in social and political disruption.\\nStill others are focused on how AI may create long-term, less obvious \\\\u201cstructural risks\\\\u201d\\\\u2014shifts to social, political and economic structures that have negative effects\\\\u2014such as destabilizing the nuclear deterrence regime.\\nMeanwhile, in the wake of AI advances, multistate and supranational bodies, individual states, industry actors, professional organizations and civil society groups have been churning out ethical principles for AI governance.\\\\n\\\\nStill, there is no agreement about AI researchers\\\\u2019 publication obligations.\\nThe Malicious Use of AI Report, OpenAI\\\\u2019s Charter and the EU High Level Expert Group on Artificial Intelligence\\\\u2019s \\\\u201cTrustworthy AI Assessment List\\\\u201d all discuss situations where limited publishing is preferable.\\nMeanwhile, individual researchers have also advocated for calculating DREAD scores\\\\u2014which weigh the potential damage, attack reliability, ease of exploit, scope of affected users and ease of discovery\\\\u2014when designing machine learning systems and outlined questions to consider before publishing.\\\\n\\\\nAt the time, Stephen Merity, a machine learning researcher, commented on the OpenAI controversy: \\\\u201cNone of us have any consensus on what we\\\\u2019re doing when it comes to responsible disclosure....\\nThis should be concerning for us all, in and out of the field.\\\\u201d\\\\n\\\\nWhat Factors Should Be Considered?\\\\n\\\\nOpenAI modeled one approach to implementing a responsible publication strategy.\\nThe company fostered a public debate by discussing and providing evidence of their program\\\\u2019s capabilities, describing what they were and were not releasing, and listing their concerns about potential misuses.\\nBy participating in a limited information-sharing regime with trusted participants, OpenAI minimized one problem of closed publication\\\\u2014that it systematically advantages larger research institutions at the expense of smaller ones, risking consolidated control of technological advances.\\\\n\\\\nHowever, OpenAI was concerned primarily with risks from intentional misuses and implicit bias (such as GPT-2\\\\u2019s tendency to associate males and he/him pronouns with the term \\\\u201ccriminal\\\\u201d).\\nIn the future, other AI researchers should consider a wider range of factors in weighing obligations for responsible publication.\\nFor would-be malicious actors, what resources are required?\\\\n\\\\nWhat is the likelihood of this harm occurring?\\nWill there be time to raise awareness, develop responses or otherwise ward off the worst effects?\\\\n\\\\nWhen might this harm be likely to occur?\\nGiven that not sharing research may create harms, what benefits or opportunities might be foregone by not publishing?\\\\n\\\\nOne critical structural question is which entity should be weighing the potential risks of a technology against its potential benefits.\\nNuclear, life sciences, cryptography and other researchers working on potentially dangerous technologies have long been discussing and developing responsible release norms that AI researchers can consider in crafting their own.\\\\n\\\\nThe Pretext Problem\\\\n\\\\nGranted, responsible publication norms may be used to support pretextual claims: An entity might overstate security concerns or other risks to justify self-interested nondisclosure.\\nBut open research can take many forms\\\\u2014including, as Nick Bostrom observed, \\\\u201copenness about science, source code, data, safety techniques, or about the capabilities, expectations, goals, plans, and governance structure of an AI project.\\\\u201d Without sharing everything, researchers can be transparent about what a system can do and their reasons for nondisclosure, so that others can weigh the commercial and security benefits of the technology against the credibility of the concerns.\\nThe irreversibility of disclosure and the unknowability of potential harms suggest favoring nondisclosure, but adherence to a strong version of the precautionary principle may transform it into a paralyzing principle, chilling the development and spread of socially beneficial technologies.\\\\n\\\\nWeighing the benefits of openness against responsible disclosure is no easy task.\\nPrecisely because political and market incentives may place undue weight on the scale in favor of immediate, concrete or concentrated benefits over long-term, abstract or diffuse risks, we need to create shared ex ante principles\\\\u2014and, eventually, institutional structures to implement and further develop them.\\nWhile it will be impossible to fully predict the future, researchers can at least increase the likelihood that these evaluations will be based on considered reasoning rather than (possibly unconscious) self-interested intuitions.\\\\n\\\\nIncentivizing Adoption\\\\n\\\\nResponsible publication norms could be integrated into the AI research process in various ways, ranging from voluntary implementation to more formal requirements.\\nThe EU Expert Group is currently testing a \\\\u201cTrustworthy AI Assessment List,\\\\u201d which requires researchers to evaluate various kinds of risks posed by their system.\\nAlternatively, researchers might engage in \\\\u201calgorithmic impact assessments\\\\u201d to consider a broader range of potential harms, as Andrew Selbst has proposed for predictive policing companies and AINow has discussed in its policy framework.\\nFor example, tax incentives can foster research in particular areas, and government contracts and philanthropic research grants could condition awards on thorough risk assessments.\\\\n\\\\nEven though OpenAI will soon be releasing the full model of GPT-2, it is critical that the conversation around thoughtful release continues, informally and formally, until the AI research community develops shared responsible publication norms.\\\\n\\\\nThanks to Hannah Bloch-Wehba, Miles Brundage, Jack Clark, Evan Selinger and Ram Shankar for conversations that informed this piece.'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /summarize\n",
    "\n",
    "article_data = []\n",
    "for article in df_ai_articles.head(5)['file_name']:\n",
    "    file = \"../data/files/\" + article\n",
    "    with open(file, \"r\") as f:\n",
    "        article_data.append(f.read())\n",
    "        \n",
    "api_endpoint = \"/summarize\"\n",
    "# URI = \"https://arcane-badlands-69055.herokuapp.com\" + api_endpoint\n",
    "URI = \"http://skynet-jr:5000\" + api_endpoint\n",
    "header = {'content-type': 'application/json'}\n",
    "r = requests.post(URI, headers=header, data=json.dumps(article_data[1]), verify=False) # <-- NOTE: Single doc\n",
    "json.loads(r.content.decode())['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = load_files_from_disk('impeachment_data/')\n",
    "NUM_TOPICS = 5\n",
    "STOPWORDS = set(stopwords.words('english')) | stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in article_data:\n",
    "    tokenized_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Gensim Dictionary - assocation word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "# for k,v in dictionary.items():\n",
    "#     print((k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "#Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = lda_model.print_topics()\n",
    "lsi_topics = lsi_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = lda_topics[0][1]\n",
    "pattern = r'\"([A-Za-z0-9_\\./\\\\-]*)\"'\n",
    "m = re.findall(pattern, t)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(lda_model.num_topics):\n",
    "    print(lda_model.print_topic(idx,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(lsi_model.num_topics):\n",
    "    print(lda_model.print_topic(idx,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 115)\n",
    "print(\"LDA Model:\")\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\"%idx, lda_model.print_topic(idx,5))\n",
    "print(\"=\" * 115)\n",
    "print(\"LSI Model:\")\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\"%idx, lsi_model.print_topic(idx,5))\n",
    "print(\"=\" * 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"\n",
    "The report puts Trump personal lawyer Rudy Giuliani at the center of a scheme to force out the U.S. ambassador to Ukraine and pressure that country’s government to investigate Joe Biden’s family and a conspiracy theory that Ukraine interfered in the 2016 U.S. election.\n",
    "The House obtained AT&T call records showing Giuliani in contact with phone numbers associated with the White House, the Office of Management and Budget, top Intelligence Committee Republican Devin Nunes, and Giuliani associate Lev Parnas. The report doesn’t say who in the White House or OMB participated in the calls.\n",
    "The calls and texts were made during the time period when Giuliani was publicly discussing his efforts to pursue investigations into the Bidens and a conspiracy theory about Ukrainian interference in the 2016 election.\n",
    "House Intelligence Chairman Adam Schiff said the call records show that “there was considerable coordination among the parties including the White House” in a smear campaign against then-U.S. Ambassador Marie Yovanovitch.\n",
    "The committee also found Giuliani in contact on Aug. 8 with phone numbers associated with the White House amid negotiations with Ukrainian officials about announcing investigations. The records also showed European Union Ambassador Gordon Sondland in contact with White House and OMB phone numbers on Aug. 9.\n",
    "One of the Sondland calls came minutes before a text message he sent saying that he thought Trump strongly wanted the “deliverable.” Sondland later said that referred to an announcement by Ukraine of investigations sought by Trump and Giuliani.\n",
    "\"\"\"\n",
    "bow = dictionary.doc2bow(clean_text(t))\n",
    "print(lsi_model[bow])\n",
    "print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = lda_index[lda_model[bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id, similarity = sims[2]\n",
    "print(article_data[document_id][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn for Topic Modeling\n",
    "\n",
    "scikit-learn offers an NMF model in addition to LDA and LSI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3, max_df=0.9, stop_words='english', lowercase=True,\n",
    "                            token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vectorized = vectorizer.fit_transform(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "print(lda_Z.shape) # tuple (num_docs, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "nmf_Z = nmf_model.fit_transform(data_vectorized)\n",
    "print(nmf_Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "lsi_Z = lsi_model.fit_transform(data_vectorized)\n",
    "print(lsi_Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_Z[0])\n",
    "print(nmf_Z[0])\n",
    "print(lsi_Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, vectorizer)\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"NMF Model:\")\n",
    "print_topics(nmf_model, vectorizer)\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    "print_topics(lsi_model, vectorizer)\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The President is underfire for his corrupt behavior. Trump Zelensky impeachment democrats volker\"\n",
    "x = nmf_model.transform(vectorizer.transform([text]))[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "output_notebook()\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "documents_2d = svd.fit_transform(data_vectorized)\n",
    " \n",
    "df = pd.DataFrame(columns=['x', 'y', 'document'])\n",
    "df['x'], df['y'], df['document'] = documents_2d[:,0], documents_2d[:,1], range(len(article_data))\n",
    " \n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"document\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2)\n",
    "words_2d = svd.fit_transform(data_vectorized.T)\n",
    "df = pd.DataFrame(columns=['x', 'y', 'document'])\n",
    "df['x'], df['y'], df['word'] = words_2d[:,0], words_2d[:,1], vectorizer.get_feature_names()\n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "LDA is the most popular method for doing topic modeling in real-world applications. That is because it provides accurate results, can be trained online (do not retrain every time we get new data) and can be run on multiple cores. Let’s repeat the process we did in the previous sections with sklearn and LatentDirichletAllocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3, max_df=0.9, stop_words='english', lowercase=True, \n",
    "                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vectorized = vectorizer.fit_transform(article_data)\n",
    "                             \n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    " \n",
    "text = \"The President is underfire for his corrupt behavior. Trump Zelensky impeachment democrats volker\"\n",
    "x = lda_model.transform(vectorizer.transform([text]))[0]\n",
    "# the largest topic contributes more to the document than the smaller topics\n",
    "print(x, x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
