{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "\n",
    "class SimpleSummarize:\n",
    "    def __init__(self, filename=None, k=None):\n",
    "        self.txt = None\n",
    "        self.word_tokens = None\n",
    "        self.sent_tokens = None\n",
    "        self.word_freq = None\n",
    "        self.freq_dist = {}\n",
    "        self.sent_scores = {}\n",
    "        self.top_sents = None\n",
    "        self.max_len = 40\n",
    "        self.summary = ''\n",
    "        self.scores = []\n",
    "        self.english_stopwords = set(stopwords.words('english')) | stop_words\n",
    "        if filename and k:\n",
    "            self.load_file_from_disk(filename)\n",
    "            self.tokenize()\n",
    "            self.word_freq_dist()\n",
    "            self.score_sentences()\n",
    "            self.summarize(k)\n",
    "    \n",
    "    def load_file_from_disk(self, filename):\n",
    "        with open(filename, \"r\") as file:\n",
    "            self.txt = file.read().replace(\"\\n\", \" \")\n",
    "            self.txt = self.txt.replace(\"\\'\",\"\")\n",
    "    \n",
    "    def tokenize(self):\n",
    "        self.word_tokens = self.tokenizer(self.txt)\n",
    "        self.sent_tokens = sent_tokenize(self.txt.lower())\n",
    "\n",
    "    def tokenizer(self,txt):\n",
    "        txt = txt.lower()\n",
    "        word_tokens = word_tokenize(txt.lower())\n",
    "        word_tokens = [w for w in word_tokens if w not in self.english_stopwords and re.match('[a-zA-Z-][a-zA-Z-]{2,}', w)]\n",
    "        return word_tokens\n",
    "    \n",
    "    def word_freq_dist(self):\n",
    "        self.word_freq = nltk.FreqDist(self.word_tokens)\n",
    "        most_freq_count = max(self.word_freq.values())\n",
    "        for k,v in self.word_freq.items():\n",
    "            self.freq_dist[k] = v/most_freq_count\n",
    "    \n",
    "    def score_sentences(self):\n",
    "        for sent in self.sent_tokens:\n",
    "            words = self.tokenizer(sent)\n",
    "            for word in words:\n",
    "                if word.lower() in self.freq_dist.keys():\n",
    "                    if len(words) < self.max_len:\n",
    "                        # if key does not exist add it and the freq_dist for the first word\n",
    "                        if sent not in self.sent_scores.keys():\n",
    "                            self.sent_scores[sent] = self.freq_dist[word.lower()]\n",
    "                        else: \n",
    "                            # the key exists and we just add the freq_dist of the following words. \n",
    "                            # We are just summing up the freq_dists for the sentence\n",
    "                            self.sent_scores[sent] += self.freq_dist[word.lower()]\n",
    "    \n",
    "    def summarize(self, k):\n",
    "        self.top_sents = Counter(self.sent_scores)\n",
    "        for t in self.top_sents.most_common(k):\n",
    "            self.summary += t[0].strip()+'. '\n",
    "            self.scores.append((t[1],t[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the judiciary committee hearing is the latest sign that house democrats are moving forward with impeachment proceedings against the president following the two-month investigation led by the house intelligence committee into allegations that trump pushed ukraine to investigate his political rivals while a white house meeting and $400 million in security aid were withheld from kiev.. the house judiciary committee has invited president donald trump or his counsel to participate in the panels first impeachment hearing next week as the house moves another step closer to impeaching the president.. read: judiciary chairman&#39;s invite to trump and his lawyers to take part in upcoming impeachment hearings the hearing announcement comes as the intelligence committee plans to release its report summarizing the findings of its investigation to the house judiciary committee soon after congress returns from its thanksgiving recess next week.. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foo = SimpleSummarize()\n",
    "# foo.load_file_from_disk(\"CNNImpeachmentArticle.txt\")\n",
    "# foo.tokenize()\n",
    "# foo.word_freq_dist()\n",
    "# foo.score_sentences()\n",
    "# foo.summarize(3)\n",
    "# foo.summary\n",
    "foo = SimpleSummarize(filename=\"CNNImpeachmentArticle.txt\", k=3)\n",
    "foo.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gensim for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3\n",
    "STOPWORDS = set(stopwords.words('english')) | stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_from_disk(data_dir):\n",
    "    text_data_list = []\n",
    "    file_list = glob(pathname=data_dir + '/*')\n",
    "    for file in file_list: \n",
    "        with open(file, \"r\") as f:\n",
    "            text_data_list.append(f.read())\n",
    "    return text_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data = load_files_from_disk('articles/')\n",
    "len(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in article_data:\n",
    "    tokenized_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Gensim Dictionary - assocation word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "# for k,v in dictionary.items():\n",
    "#     print((k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 1)]\n",
      "[(0, 1), (18, 1), (39, 1), (49, 1), (52, 2), (70, 2), (71, 1), (74, 1), (81, 1), (84, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[0][:5])\n",
    "print(corpus[20][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "#Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "LDA Model:\n",
      "Topic #0: 0.006*\"like\" + 0.004*\"people\" + 0.004*\"new\" + 0.004*\"football\" + 0.004*\"time\" + 0.004*\"game\" + 0.003*\"way\" + 0.003*\"make\" + 0.002*\"data\" + 0.002*\"years\"\n",
      "Topic #1: 0.005*\"like\" + 0.005*\"people\" + 0.004*\"data\" + 0.003*\"new\" + 0.003*\"handstand\" + 0.003*\"facebook\" + 0.003*\"time\" + 0.003*\"make\" + 0.002*\"use\" + 0.002*\"big\"\n",
      "Topic #2: 0.006*\"new\" + 0.005*\"like\" + 0.003*\"time\" + 0.003*\"make\" + 0.003*\"pixels\" + 0.003*\"people\" + 0.003*\"buffer\" + 0.003*\"facebook\" + 0.002*\"use\" + 0.002*\"image\"\n",
      "===================================================================================================================\n",
      "LSI Model:\n",
      "Topic #0: -0.315*\"football\" + -0.266*\"like\" + -0.263*\"game\" + -0.253*\"people\" + -0.203*\"facebook\" + -0.181*\"new\" + -0.159*\"time\" + -0.140*\"targeting\" + -0.132*\"use\" + -0.114*\"make\"\n",
      "Topic #1: -0.418*\"football\" + -0.344*\"game\" + 0.319*\"pixels\" + 0.257*\"image\" + 0.241*\"facebook\" + 0.169*\"sharing\" + 0.166*\"images\" + -0.148*\"players\" + 0.122*\"people\" + 0.113*\"targeting\"\n",
      "Topic #2: 0.443*\"pixels\" + 0.346*\"image\" + -0.248*\"people\" + 0.241*\"sharing\" + 0.233*\"football\" + 0.232*\"images\" + 0.189*\"game\" + -0.174*\"targeting\" + 0.134*\"ideal\" + 0.126*\"wide\"\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 115)\n",
    "print(\"LDA Model:\")\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\"%idx, lda_model.print_topic(idx,10))\n",
    "print(\"=\" * 115)\n",
    "print(\"LSI Model:\")\n",
    "for idx in range(NUM_TOPICS):\n",
    "    print(\"Topic #%s:\"%idx, lsi_model.print_topic(idx,10))\n",
    "print(\"=\" * 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
